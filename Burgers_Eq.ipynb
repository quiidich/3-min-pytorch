{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtKmaemRxZdSx+fjv+SvWc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/quiidich/3-min-pytorch/blob/master/Burgers_Eq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "76PCXI9TfvS5"
      },
      "outputs": [],
      "source": [
        "from random import uniform\n",
        "from functools import partial\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "iter = 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed: int = 42):\n",
        "    '''\n",
        "    Seeding the random variables for reproducibility\n",
        "    '''\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "-TP7wsePf0sl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BurgersNN(nn.Module):\n",
        "    def __init__(self,):\n",
        "        super(BurgersNN, self).__init__()\n",
        "        # Input layer\n",
        "        self.linear_in = nn.Linear(2, 20)\n",
        "        # Output layer\n",
        "        self.linear_out = nn.Linear(20, 1)\n",
        "        # Hidden layers\n",
        "        self.layers = nn.ModuleList(\n",
        "            [nn.Linear(20, 20) for i in range(8)]\n",
        "        )\n",
        "        # Activation function\n",
        "        self.act = nn.Tanh()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.linear_in(x)\n",
        "        x = self.act(x)\n",
        "        for layer in self.layers:\n",
        "            x = self.act(layer(x))\n",
        "        x = self.linear_out(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "5wY_7W-Yf3md"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def derivative(dy: torch.Tensor, x: torch.Tensor, order: int = 1) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    This function calculates the derivative of the model at x_f\n",
        "    \"\"\"\n",
        "    for i in range(order):\n",
        "        dy = torch.autograd.grad(\n",
        "            dy, x, grad_outputs = torch.ones_like(dy), create_graph=True, retain_graph=True\n",
        "        )[0]\n",
        "    return dy"
      ],
      "metadata": {
        "id": "uaVUs5kEf-4O"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def u_function(model: BurgersNN, x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    This function evaluates the model on the input x\n",
        "    \"\"\"\n",
        "    model_input = torch.stack((x, t), axis = 1)\n",
        "    return model(model_input)"
      ],
      "metadata": {
        "id": "TcuAYYQXgCN1"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f(model, x_f, t_f):\n",
        "    u = u_function(model, x_f, t_f)\n",
        "    u_t = derivative(u, t_f, order=1)\n",
        "    u_x = derivative(u, x_f, order=1)\n",
        "    u_xx = derivative(u, x_f, order=2)\n",
        "    f = u_t + u.T*u_x - (0.01/np.pi)*u_xx\n",
        "    return f"
      ],
      "metadata": {
        "id": "ao6fIzVBgFCO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(model: BurgersNN, x_u: torch.Tensor, x_f: torch.Tensor, t_f: torch.Tensor, t_u: torch.Tensor, y_u: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    This function evaluates the physics governing the model on the input x_f\n",
        "    \"\"\"\n",
        "    u = u_function(model, x_f, t_f)\n",
        "    MSE_f = f(model, x_f, t_f).pow(2).mean()\n",
        "    MSE_u = (u_function(model, x_u, t_u)-y_u).pow(2).mean()\n",
        "    return MSE_f + MSE_u"
      ],
      "metadata": {
        "id": "agyuFeKQgHde"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "    if type(m) == nn.Linear:\n",
        "        torch.nn.init.xavier_normal_(m.weight)\n",
        "        m.bias.data.fill_(0.01)"
      ],
      "metadata": {
        "id": "ZhGJWjBvgK2-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def closure(model: BurgersNN, optimizer, X_u_train: torch.Tensor, X_f_train:torch.Tensor, Y_u_train: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    In order to use the LBFGS optimizer, we need to define a closure function. This function is called by the optimizer\n",
        "    and the optimizer contains the inner loop for the optimization and it continues until the tolerance is met.\n",
        "    \"\"\"\n",
        "    x_u = X_u_train[:, 0]\n",
        "    t_u = X_u_train[:, 1]\n",
        "    x_f = X_f_train[:, 0]\n",
        "    t_f = X_f_train[:, 1]\n",
        "    y_u = Y_u_train\n",
        "    optimizer.zero_grad()\n",
        "    loss = loss_function(model, x_u, x_f, t_f, t_u, y_u)\n",
        "    loss.backward()\n",
        "    global iter\n",
        "    iter += 1\n",
        "    print(f\" iteration: {iter}  loss: {loss.item()}\")\n",
        "    return loss"
      ],
      "metadata": {
        "id": "Sk7tH6dPgNal"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, X_u_train, X_f_train, u_train):\n",
        "    # Initialize the optimizer\n",
        "    optimizer = torch.optim.LBFGS(model.parameters(),\n",
        "                                    lr=1,\n",
        "                                    max_iter=50000,\n",
        "                                    max_eval=50000,\n",
        "                                    history_size=50,\n",
        "                                    tolerance_grad=1e-05,\n",
        "                                    tolerance_change=0.5 * np.finfo(float).eps,\n",
        "                                    line_search_fn=\"strong_wolfe\")\n",
        "\n",
        "    # the optimizer.step requires the closure function to be a callable function without inputs\n",
        "    # therefore we need to define a partial function and pass it to the optimizer\n",
        "    closure_fn = partial(closure, model, optimizer, X_u_train, X_f_train, u_train)\n",
        "    optimizer.step(closure_fn)"
      ],
      "metadata": {
        "id": "I8C-YsJCgQgu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Set seed for reproducibility\n",
        "    set_seed(42)\n",
        "\n",
        "    nu = 0.01 / np.pi         # constant in the diff. equation\n",
        "    N_u = 100                 # number of data points in the boundaries\n",
        "    N_f = 10000               # number of collocation points\n",
        "\n",
        "    # X_u_train: a set of pairs (x, t) located at:\n",
        "        # x =  1, t = [0,  1]\n",
        "        # x = -1, t = [0,  1]\n",
        "        # t =  0, x = [-1, 1]\n",
        "    x_upper = np.ones((N_u//4, 1), dtype=float)\n",
        "    x_lower = np.ones((N_u//4, 1), dtype=float) * (-1)\n",
        "    t_zero = np.zeros((N_u//2, 1), dtype=float)\n",
        "\n",
        "    t_upper = np.random.rand(N_u//4, 1)\n",
        "    t_lower = np.random.rand(N_u//4, 1)\n",
        "    x_zero = (-1) + np.random.rand(N_u//2, 1) * (1 - (-1))\n",
        "\n",
        "    # stack uppers, lowers and zeros:\n",
        "    X_upper = np.hstack( (x_upper, t_upper) )\n",
        "    X_lower = np.hstack( (x_lower, t_lower) )\n",
        "    X_zero = np.hstack( (x_zero, t_zero) )\n",
        "\n",
        "    # each one of these three arrays haS 2 columns,\n",
        "    # now we stack them vertically, the resulting array will also have 2\n",
        "    # columns and 100 rows:\n",
        "    X_u_train = np.vstack( (X_upper, X_lower, X_zero) )\n",
        "\n",
        "    # shuffle X_u_train:\n",
        "    index = np.arange(0, N_u)\n",
        "    np.random.shuffle(index)\n",
        "    X_u_train = X_u_train[index, :]\n",
        "\n",
        "    # make X_f_train:\n",
        "    X_f_train = np.zeros((N_f, 2), dtype=float)\n",
        "    for row in range(N_f):\n",
        "        x = uniform(-1, 1)  # x range\n",
        "        t = uniform( 0, 1)  # t range\n",
        "\n",
        "        X_f_train[row, 0] = x\n",
        "        X_f_train[row, 1] = t\n",
        "\n",
        "    # add the boundary points to the collocation points:\n",
        "    X_f_train = np.vstack( (X_f_train, X_u_train) )\n",
        "\n",
        "    # make u_train\n",
        "    u_upper =  np.zeros((N_u//4, 1), dtype=float)\n",
        "    u_lower =  np.zeros((N_u//4, 1), dtype=float)\n",
        "    u_zero = -np.sin(np.pi * x_zero)\n",
        "\n",
        "    # stack them in the same order as X_u_train was stacked:\n",
        "    u_train = np.vstack( (u_upper, u_lower, u_zero) )\n",
        "\n",
        "    # match indices with X_u_train\n",
        "    u_train = u_train[index, :]\n",
        "    # Model instantiation\n",
        "    model = BurgersNN()\n",
        "    model.apply(init_weights)\n",
        "    # Training\n",
        "    X_u_train = torch.from_numpy(X_u_train).requires_grad_(True).float()\n",
        "    X_f_train = torch.from_numpy(X_f_train).requires_grad_(True).float()\n",
        "    u_train = torch.from_numpy(u_train).requires_grad_(True).float()\n",
        "\n",
        "    model.train()\n",
        "    train(model, X_u_train, X_f_train, u_train)\n",
        "    # save the model\n",
        "    torch.save(model.state_dict(), 'Burgers_Equation/models/model_LBFGS_shuffle_normal.pt')\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g5ZJpPDtgVOV",
        "outputId": "0c3c53db-f308-4cb0-b2e5-8e24976f312f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " iteration: 1  loss: 0.2293640524148941\n",
            " iteration: 2  loss: 0.21785911917686462\n",
            " iteration: 3  loss: 0.20473024249076843\n",
            " iteration: 4  loss: 0.19932690262794495\n",
            " iteration: 5  loss: 0.1981697678565979\n",
            " iteration: 6  loss: 0.1948550045490265\n",
            " iteration: 7  loss: 0.1793188750743866\n",
            " iteration: 8  loss: 0.19664618372917175\n",
            " iteration: 9  loss: 0.1724817156791687\n",
            " iteration: 10  loss: 0.16531942784786224\n",
            " iteration: 11  loss: 0.1623375117778778\n",
            " iteration: 12  loss: 0.15767674148082733\n",
            " iteration: 13  loss: 0.15157198905944824\n",
            " iteration: 14  loss: 0.15005958080291748\n",
            " iteration: 15  loss: 0.1489448994398117\n",
            " iteration: 16  loss: 0.14304740726947784\n",
            " iteration: 17  loss: 0.13657788932323456\n",
            " iteration: 18  loss: 2.2932674884796143\n",
            " iteration: 19  loss: 0.13524307310581207\n",
            " iteration: 20  loss: 0.132037952542305\n",
            " iteration: 21  loss: 0.12714479863643646\n",
            " iteration: 22  loss: 0.12551532685756683\n",
            " iteration: 23  loss: 0.12286354601383209\n",
            " iteration: 24  loss: 0.12094718962907791\n",
            " iteration: 25  loss: 0.1190013587474823\n",
            " iteration: 26  loss: 0.1181078553199768\n",
            " iteration: 27  loss: 0.11618568003177643\n",
            " iteration: 28  loss: 0.10905884206295013\n",
            " iteration: 29  loss: 0.10374684631824493\n",
            " iteration: 30  loss: 0.10090813040733337\n",
            " iteration: 31  loss: 0.09965196996927261\n",
            " iteration: 32  loss: 0.09947777539491653\n",
            " iteration: 33  loss: 0.0987662747502327\n",
            " iteration: 34  loss: 0.09816265106201172\n",
            " iteration: 35  loss: 0.09742368757724762\n",
            " iteration: 36  loss: 0.096575066447258\n",
            " iteration: 37  loss: 0.09574773907661438\n",
            " iteration: 38  loss: 0.09515825659036636\n",
            " iteration: 39  loss: 0.0943630039691925\n",
            " iteration: 40  loss: 0.09393277764320374\n",
            " iteration: 41  loss: 0.09355904906988144\n",
            " iteration: 42  loss: 0.0930924341082573\n",
            " iteration: 43  loss: 0.09104938805103302\n",
            " iteration: 44  loss: 0.09077522158622742\n",
            " iteration: 45  loss: 0.09014473855495453\n",
            " iteration: 46  loss: 0.08972764015197754\n",
            " iteration: 47  loss: 0.08884849399328232\n",
            " iteration: 48  loss: 0.08719232678413391\n",
            " iteration: 49  loss: 0.08581605553627014\n",
            " iteration: 50  loss: 0.08410130441188812\n",
            " iteration: 51  loss: 0.08339622616767883\n",
            " iteration: 52  loss: 0.0830707773566246\n",
            " iteration: 53  loss: 0.08281788229942322\n",
            " iteration: 54  loss: 0.08250986039638519\n",
            " iteration: 55  loss: 0.08186011016368866\n",
            " iteration: 56  loss: 0.08078940212726593\n",
            " iteration: 57  loss: 0.07938628643751144\n",
            " iteration: 58  loss: 0.07776182144880295\n",
            " iteration: 59  loss: 0.07690010219812393\n",
            " iteration: 60  loss: 0.07643955945968628\n",
            " iteration: 61  loss: 0.07622422277927399\n",
            " iteration: 62  loss: 0.0756755918264389\n",
            " iteration: 63  loss: 0.07480284571647644\n",
            " iteration: 64  loss: 0.07330448925495148\n",
            " iteration: 65  loss: 0.07146728783845901\n",
            " iteration: 66  loss: 0.06844954192638397\n",
            " iteration: 67  loss: 0.06587034463882446\n",
            " iteration: 68  loss: 0.06462933868169785\n",
            " iteration: 69  loss: 0.0641615092754364\n",
            " iteration: 70  loss: 0.06361694633960724\n",
            " iteration: 71  loss: 0.06306208670139313\n",
            " iteration: 72  loss: 0.0627254843711853\n",
            " iteration: 73  loss: 0.0624956339597702\n",
            " iteration: 74  loss: 0.06223905831575394\n",
            " iteration: 75  loss: 0.061810389161109924\n",
            " iteration: 76  loss: 0.0607740581035614\n",
            " iteration: 77  loss: 0.0603586807847023\n",
            " iteration: 78  loss: 0.059484198689460754\n",
            " iteration: 79  loss: 0.05903539061546326\n",
            " iteration: 80  loss: 0.05853218585252762\n",
            " iteration: 81  loss: 0.05787079408764839\n",
            " iteration: 82  loss: 0.057344406843185425\n",
            " iteration: 83  loss: 0.05661391466856003\n",
            " iteration: 84  loss: 0.05541885271668434\n",
            " iteration: 85  loss: 0.0547739714384079\n",
            " iteration: 86  loss: 0.054513104259967804\n",
            " iteration: 87  loss: 0.0543200708925724\n",
            " iteration: 88  loss: 0.05409611016511917\n",
            " iteration: 89  loss: 0.05374103784561157\n",
            " iteration: 90  loss: 0.05282912030816078\n",
            " iteration: 91  loss: 0.05162972956895828\n",
            " iteration: 92  loss: 0.05203697457909584\n",
            " iteration: 93  loss: 0.050801921635866165\n",
            " iteration: 94  loss: 0.04980713129043579\n",
            " iteration: 95  loss: 0.048609547317028046\n",
            " iteration: 96  loss: 0.04802360758185387\n",
            " iteration: 97  loss: 0.04724615812301636\n",
            " iteration: 98  loss: 0.04604987055063248\n",
            " iteration: 99  loss: 0.04452682286500931\n",
            " iteration: 100  loss: 0.04366803169250488\n",
            " iteration: 101  loss: 0.04291081428527832\n",
            " iteration: 102  loss: 0.04235856980085373\n",
            " iteration: 103  loss: 0.041951633989810944\n",
            " iteration: 104  loss: 0.0414450541138649\n",
            " iteration: 105  loss: 0.04098502919077873\n",
            " iteration: 106  loss: 0.04060545191168785\n",
            " iteration: 107  loss: 0.04038752615451813\n",
            " iteration: 108  loss: 0.04017842933535576\n",
            " iteration: 109  loss: 0.03982311114668846\n",
            " iteration: 110  loss: 0.03969770297408104\n",
            " iteration: 111  loss: 0.03955072909593582\n",
            " iteration: 112  loss: 0.04044433310627937\n",
            " iteration: 113  loss: 0.039501436054706573\n",
            " iteration: 114  loss: 0.03912908583879471\n",
            " iteration: 115  loss: 0.03876849263906479\n",
            " iteration: 116  loss: 0.038349494338035583\n",
            " iteration: 117  loss: 0.03785356879234314\n",
            " iteration: 118  loss: 0.037311702966690063\n",
            " iteration: 119  loss: 0.03701286390423775\n",
            " iteration: 120  loss: 0.036748893558979034\n",
            " iteration: 121  loss: 0.03642292693257332\n",
            " iteration: 122  loss: 0.035780809819698334\n",
            " iteration: 123  loss: 0.03428370878100395\n",
            " iteration: 124  loss: 0.03294780105352402\n",
            " iteration: 125  loss: 0.03320373222231865\n",
            " iteration: 126  loss: 0.03251928091049194\n",
            " iteration: 127  loss: 0.03218958526849747\n",
            " iteration: 128  loss: 0.03212478756904602\n",
            " iteration: 129  loss: 0.03190268576145172\n",
            " iteration: 130  loss: 0.0317547544836998\n",
            " iteration: 131  loss: 0.03152918815612793\n",
            " iteration: 132  loss: 0.031139271333813667\n",
            " iteration: 133  loss: 0.030549321323633194\n",
            " iteration: 134  loss: 0.02960335835814476\n",
            " iteration: 135  loss: 0.029144492000341415\n",
            " iteration: 136  loss: 0.02834082953631878\n",
            " iteration: 137  loss: 0.028160275891423225\n",
            " iteration: 138  loss: 0.02802092954516411\n",
            " iteration: 139  loss: 0.0279228538274765\n",
            " iteration: 140  loss: 0.027793679386377335\n",
            " iteration: 141  loss: 0.02749069221317768\n",
            " iteration: 142  loss: 0.027144767343997955\n",
            " iteration: 143  loss: 0.026666652411222458\n",
            " iteration: 144  loss: 0.025900136679410934\n",
            " iteration: 145  loss: 0.02518293261528015\n",
            " iteration: 146  loss: 0.02432839199900627\n",
            " iteration: 147  loss: 0.02353166788816452\n",
            " iteration: 148  loss: 0.02324127033352852\n",
            " iteration: 149  loss: 0.02312910184264183\n",
            " iteration: 150  loss: 0.023064618930220604\n",
            " iteration: 151  loss: 0.02298194356262684\n",
            " iteration: 152  loss: 0.022877749055624008\n",
            " iteration: 153  loss: 0.02271602675318718\n",
            " iteration: 154  loss: 0.022379688918590546\n",
            " iteration: 155  loss: 0.022042134776711464\n",
            " iteration: 156  loss: 0.021637767553329468\n",
            " iteration: 157  loss: 0.021455861628055573\n",
            " iteration: 158  loss: 0.02126011624932289\n",
            " iteration: 159  loss: 0.021124999970197678\n",
            " iteration: 160  loss: 0.020919740200042725\n",
            " iteration: 161  loss: 0.020781246945261955\n",
            " iteration: 162  loss: 0.020493127405643463\n",
            " iteration: 163  loss: 0.02026083506643772\n",
            " iteration: 164  loss: 0.020051416009664536\n",
            " iteration: 165  loss: 0.019726764410734177\n",
            " iteration: 166  loss: 0.01940516196191311\n",
            " iteration: 167  loss: 0.019156021997332573\n",
            " iteration: 168  loss: 0.01885400339961052\n",
            " iteration: 169  loss: 0.01868058368563652\n",
            " iteration: 170  loss: 0.018586572259664536\n",
            " iteration: 171  loss: 0.018341880291700363\n",
            " iteration: 172  loss: 0.018035322427749634\n",
            " iteration: 173  loss: 0.017376411706209183\n",
            " iteration: 174  loss: 0.016860734671354294\n",
            " iteration: 175  loss: 0.016607200726866722\n",
            " iteration: 176  loss: 0.016796812415122986\n",
            " iteration: 177  loss: 0.016341539099812508\n",
            " iteration: 178  loss: 0.01610126905143261\n",
            " iteration: 179  loss: 0.015990789979696274\n",
            " iteration: 180  loss: 0.01591075398027897\n",
            " iteration: 181  loss: 0.015831682831048965\n",
            " iteration: 182  loss: 0.015737567096948624\n",
            " iteration: 183  loss: 0.015615412034094334\n",
            " iteration: 184  loss: 0.015488900244235992\n",
            " iteration: 185  loss: 0.015403042547404766\n",
            " iteration: 186  loss: 0.015229927375912666\n",
            " iteration: 187  loss: 0.015017477795481682\n",
            " iteration: 188  loss: 0.014835481531918049\n",
            " iteration: 189  loss: 0.014609551057219505\n",
            " iteration: 190  loss: 0.014256026595830917\n",
            " iteration: 191  loss: 0.013867588713765144\n",
            " iteration: 192  loss: 0.013651065528392792\n",
            " iteration: 193  loss: 0.013485338538885117\n",
            " iteration: 194  loss: 0.01335656363517046\n",
            " iteration: 195  loss: 0.013266317546367645\n",
            " iteration: 196  loss: 0.013037065044045448\n",
            " iteration: 197  loss: 0.01325792632997036\n",
            " iteration: 198  loss: 0.01290016621351242\n",
            " iteration: 199  loss: 0.012603085488080978\n",
            " iteration: 200  loss: 0.012501215562224388\n",
            " iteration: 201  loss: 0.012390426360070705\n",
            " iteration: 202  loss: 0.012079067528247833\n",
            " iteration: 203  loss: 0.011949753388762474\n",
            " iteration: 204  loss: 0.0118145402520895\n",
            " iteration: 205  loss: 0.011720307171344757\n",
            " iteration: 206  loss: 0.011612394824624062\n",
            " iteration: 207  loss: 0.011495589278638363\n",
            " iteration: 208  loss: 0.011404848657548428\n",
            " iteration: 209  loss: 0.011259390041232109\n",
            " iteration: 210  loss: 0.01119265891611576\n",
            " iteration: 211  loss: 0.011074148118495941\n",
            " iteration: 212  loss: 0.011009725742042065\n",
            " iteration: 213  loss: 0.010946853086352348\n",
            " iteration: 214  loss: 0.01088330615311861\n",
            " iteration: 215  loss: 0.010773174464702606\n",
            " iteration: 216  loss: 0.010652469471096992\n",
            " iteration: 217  loss: 0.010498963296413422\n",
            " iteration: 218  loss: 0.010374575853347778\n",
            " iteration: 219  loss: 0.01024203933775425\n",
            " iteration: 220  loss: 0.010139191523194313\n",
            " iteration: 221  loss: 0.010042608715593815\n",
            " iteration: 222  loss: 0.009974773041903973\n",
            " iteration: 223  loss: 0.009900780394673347\n",
            " iteration: 224  loss: 0.009825767017900944\n",
            " iteration: 225  loss: 0.009706908836960793\n",
            " iteration: 226  loss: 0.009545082226395607\n",
            " iteration: 227  loss: 0.009408173151314259\n",
            " iteration: 228  loss: 0.009337983094155788\n",
            " iteration: 229  loss: 0.009306837804615498\n",
            " iteration: 230  loss: 0.009268531575798988\n",
            " iteration: 231  loss: 0.009148615412414074\n",
            " iteration: 232  loss: 0.008992249146103859\n",
            " iteration: 233  loss: 0.008827967569231987\n",
            " iteration: 234  loss: 0.00877685658633709\n",
            " iteration: 235  loss: 0.008579891175031662\n",
            " iteration: 236  loss: 0.008533075451850891\n",
            " iteration: 237  loss: 0.00848366692662239\n",
            " iteration: 238  loss: 0.008447282016277313\n",
            " iteration: 239  loss: 0.008406534790992737\n",
            " iteration: 240  loss: 0.008329284377396107\n",
            " iteration: 241  loss: 0.008180858567357063\n",
            " iteration: 242  loss: 0.0080658458173275\n",
            " iteration: 243  loss: 0.007962487637996674\n",
            " iteration: 244  loss: 0.007865563035011292\n",
            " iteration: 245  loss: 0.007793155498802662\n",
            " iteration: 246  loss: 0.007707340642809868\n",
            " iteration: 247  loss: 0.007653476670384407\n",
            " iteration: 248  loss: 0.00761403888463974\n",
            " iteration: 249  loss: 0.007569550536572933\n",
            " iteration: 250  loss: 0.007519122678786516\n",
            " iteration: 251  loss: 0.007482435554265976\n",
            " iteration: 252  loss: 0.00757343927398324\n",
            " iteration: 253  loss: 0.007465658709406853\n",
            " iteration: 254  loss: 0.007438631262630224\n",
            " iteration: 255  loss: 0.007413880899548531\n",
            " iteration: 256  loss: 0.007391191553324461\n",
            " iteration: 257  loss: 0.00736161507666111\n",
            " iteration: 258  loss: 0.007322775200009346\n",
            " iteration: 259  loss: 0.00728048337623477\n",
            " iteration: 260  loss: 0.007189885713160038\n",
            " iteration: 261  loss: 0.007103876210749149\n",
            " iteration: 262  loss: 0.0070207021199166775\n",
            " iteration: 263  loss: 0.006947280839085579\n",
            " iteration: 264  loss: 0.006863880902528763\n",
            " iteration: 265  loss: 0.006778169423341751\n",
            " iteration: 266  loss: 0.006704588420689106\n",
            " iteration: 267  loss: 0.006637913174927235\n",
            " iteration: 268  loss: 0.006596572697162628\n",
            " iteration: 269  loss: 0.0065415846183896065\n",
            " iteration: 270  loss: 0.006627188995480537\n",
            " iteration: 271  loss: 0.006493656896054745\n",
            " iteration: 272  loss: 0.0064309705048799515\n",
            " iteration: 273  loss: 0.006329169496893883\n",
            " iteration: 274  loss: 0.006264575757086277\n",
            " iteration: 275  loss: 0.0061675566248595715\n",
            " iteration: 276  loss: 0.00606371508911252\n",
            " iteration: 277  loss: 0.005993864964693785\n",
            " iteration: 278  loss: 0.005957692861557007\n",
            " iteration: 279  loss: 0.0059629688039422035\n",
            " iteration: 280  loss: 0.005928093567490578\n",
            " iteration: 281  loss: 0.005903569050133228\n",
            " iteration: 282  loss: 0.005870955064892769\n",
            " iteration: 283  loss: 0.00583627400919795\n",
            " iteration: 284  loss: 0.005765148904174566\n",
            " iteration: 285  loss: 0.005679839290678501\n",
            " iteration: 286  loss: 0.005745030473917723\n",
            " iteration: 287  loss: 0.005607832223176956\n",
            " iteration: 288  loss: 0.005555519834160805\n",
            " iteration: 289  loss: 0.005514503922313452\n",
            " iteration: 290  loss: 0.005489273462444544\n",
            " iteration: 291  loss: 0.005451641511172056\n",
            " iteration: 292  loss: 0.005406572483479977\n",
            " iteration: 293  loss: 0.00538643728941679\n",
            " iteration: 294  loss: 0.0053785415366292\n",
            " iteration: 295  loss: 0.0053701261058449745\n",
            " iteration: 296  loss: 0.005350432824343443\n",
            " iteration: 297  loss: 0.005335899535566568\n",
            " iteration: 298  loss: 0.00531425978988409\n",
            " iteration: 299  loss: 0.005269605200737715\n",
            " iteration: 300  loss: 0.005239960737526417\n",
            " iteration: 301  loss: 0.005183940753340721\n",
            " iteration: 302  loss: 0.00511143309995532\n",
            " iteration: 303  loss: 0.005063606426119804\n",
            " iteration: 304  loss: 0.004989384673535824\n",
            " iteration: 305  loss: 0.0049340990372002125\n",
            " iteration: 306  loss: 0.004869470372796059\n",
            " iteration: 307  loss: 0.004839373752474785\n",
            " iteration: 308  loss: 0.004800192080438137\n",
            " iteration: 309  loss: 0.0047676959075033665\n",
            " iteration: 310  loss: 0.004725881386548281\n",
            " iteration: 311  loss: 0.004706243984401226\n",
            " iteration: 312  loss: 0.004654772579669952\n",
            " iteration: 313  loss: 0.004596037790179253\n",
            " iteration: 314  loss: 0.004512845538556576\n",
            " iteration: 315  loss: 0.004460870288312435\n",
            " iteration: 316  loss: 0.004574840422719717\n",
            " iteration: 317  loss: 0.004445831291377544\n",
            " iteration: 318  loss: 0.004423795733600855\n",
            " iteration: 319  loss: 0.004409534856677055\n",
            " iteration: 320  loss: 0.004396362230181694\n",
            " iteration: 321  loss: 0.004369786009192467\n",
            " iteration: 322  loss: 0.004394655581563711\n",
            " iteration: 323  loss: 0.0043540289625525475\n",
            " iteration: 324  loss: 0.00431949645280838\n",
            " iteration: 325  loss: 0.004255715757608414\n",
            " iteration: 326  loss: 0.0041746580973267555\n",
            " iteration: 327  loss: 0.004186227917671204\n",
            " iteration: 328  loss: 0.004109677392989397\n",
            " iteration: 329  loss: 0.004019472282379866\n",
            " iteration: 330  loss: 0.0039274971932172775\n",
            " iteration: 331  loss: 0.003794086631387472\n",
            " iteration: 332  loss: 0.00373272062279284\n",
            " iteration: 333  loss: 0.003695025807246566\n",
            " iteration: 334  loss: 0.003709046635776758\n",
            " iteration: 335  loss: 0.0036766892299056053\n",
            " iteration: 336  loss: 0.003661709139123559\n",
            " iteration: 337  loss: 0.0036521379370242357\n",
            " iteration: 338  loss: 0.0036443485878407955\n",
            " iteration: 339  loss: 0.003639928298071027\n",
            " iteration: 340  loss: 0.0036287549883127213\n",
            " iteration: 341  loss: 0.0036225481890141964\n",
            " iteration: 342  loss: 0.003614847082644701\n",
            " iteration: 343  loss: 0.003602842101827264\n",
            " iteration: 344  loss: 0.0035771294496953487\n",
            " iteration: 345  loss: 0.003548830281943083\n",
            " iteration: 346  loss: 0.0035065351985394955\n",
            " iteration: 347  loss: 0.0034672124311327934\n",
            " iteration: 348  loss: 0.003449179930612445\n",
            " iteration: 349  loss: 0.0033811586908996105\n",
            " iteration: 350  loss: 0.0033249822445213795\n",
            " iteration: 351  loss: 0.0032678672578185797\n",
            " iteration: 352  loss: 0.0032196473330259323\n",
            " iteration: 353  loss: 0.0032019109930843115\n",
            " iteration: 354  loss: 0.0031919421162456274\n",
            " iteration: 355  loss: 0.0031851776875555515\n",
            " iteration: 356  loss: 0.0031770355999469757\n",
            " iteration: 357  loss: 0.0031668799929320812\n",
            " iteration: 358  loss: 0.00315010454505682\n",
            " iteration: 359  loss: 0.0031236796639859676\n",
            " iteration: 360  loss: 0.0030897059477865696\n",
            " iteration: 361  loss: 0.003290999447926879\n",
            " iteration: 362  loss: 0.0030731107108294964\n",
            " iteration: 363  loss: 0.003051530569791794\n",
            " iteration: 364  loss: 0.0030355299822986126\n",
            " iteration: 365  loss: 0.003018222749233246\n",
            " iteration: 366  loss: 0.0029943424742668867\n",
            " iteration: 367  loss: 0.0029634558595716953\n",
            " iteration: 368  loss: 0.0029241773299872875\n",
            " iteration: 369  loss: 0.003069649450480938\n",
            " iteration: 370  loss: 0.002880576765164733\n",
            " iteration: 371  loss: 0.0028865369968116283\n",
            " iteration: 372  loss: 0.002843038644641638\n",
            " iteration: 373  loss: 0.002805656287819147\n",
            " iteration: 374  loss: 0.0027866382151842117\n",
            " iteration: 375  loss: 0.002753917360678315\n",
            " iteration: 376  loss: 0.0027342038229107857\n",
            " iteration: 377  loss: 0.0027126362547278404\n",
            " iteration: 378  loss: 0.0026816916652023792\n",
            " iteration: 379  loss: 0.0026469496078789234\n",
            " iteration: 380  loss: 0.0026179933920502663\n",
            " iteration: 381  loss: 0.002589112613350153\n",
            " iteration: 382  loss: 0.0025740298442542553\n",
            " iteration: 383  loss: 0.0025458885356783867\n",
            " iteration: 384  loss: 0.0025352640077471733\n",
            " iteration: 385  loss: 0.0024953470565378666\n",
            " iteration: 386  loss: 0.002474884269759059\n",
            " iteration: 387  loss: 0.0024871635250747204\n",
            " iteration: 388  loss: 0.0024444509763270617\n",
            " iteration: 389  loss: 0.0024043279699981213\n",
            " iteration: 390  loss: 0.002378119621425867\n",
            " iteration: 391  loss: 0.0023524907883256674\n",
            " iteration: 392  loss: 0.0023415102623403072\n",
            " iteration: 393  loss: 0.0023209508508443832\n",
            " iteration: 394  loss: 0.0023067910224199295\n",
            " iteration: 395  loss: 0.00229164632037282\n",
            " iteration: 396  loss: 0.0022796038538217545\n",
            " iteration: 397  loss: 0.0022848709486424923\n",
            " iteration: 398  loss: 0.002271752804517746\n",
            " iteration: 399  loss: 0.0022582230158150196\n",
            " iteration: 400  loss: 0.0022500138729810715\n",
            " iteration: 401  loss: 0.0022388859651982784\n",
            " iteration: 402  loss: 0.002232518745586276\n",
            " iteration: 403  loss: 0.002223673975095153\n",
            " iteration: 404  loss: 0.0022174264304339886\n",
            " iteration: 405  loss: 0.0022124589886516333\n",
            " iteration: 406  loss: 0.0022089900448918343\n",
            " iteration: 407  loss: 0.0022015052381902933\n",
            " iteration: 408  loss: 0.0021927575580775738\n",
            " iteration: 409  loss: 0.0021613475400954485\n",
            " iteration: 410  loss: 0.0021414023358374834\n",
            " iteration: 411  loss: 0.007486904039978981\n",
            " iteration: 412  loss: 0.0021387720480561256\n",
            " iteration: 413  loss: 0.002096169628202915\n",
            " iteration: 414  loss: 0.002064950531348586\n",
            " iteration: 415  loss: 0.0020492305047810078\n",
            " iteration: 416  loss: 0.0020383093506097794\n",
            " iteration: 417  loss: 0.0020285719074308872\n",
            " iteration: 418  loss: 0.002014636993408203\n",
            " iteration: 419  loss: 0.002003715606406331\n",
            " iteration: 420  loss: 0.001989402575418353\n",
            " iteration: 421  loss: 0.001981515670195222\n",
            " iteration: 422  loss: 0.001975193852558732\n",
            " iteration: 423  loss: 0.001969442702829838\n",
            " iteration: 424  loss: 0.0019471420673653483\n",
            " iteration: 425  loss: 0.0022926677484065294\n",
            " iteration: 426  loss: 0.0019387719221413136\n",
            " iteration: 427  loss: 0.0019254118669778109\n",
            " iteration: 428  loss: 0.0019073737785220146\n",
            " iteration: 429  loss: 0.0018966237548738718\n",
            " iteration: 430  loss: 0.0018967243377119303\n",
            " iteration: 431  loss: 0.0018903189338743687\n",
            " iteration: 432  loss: 0.001893398817628622\n",
            " iteration: 433  loss: 0.0018849814077839255\n",
            " iteration: 434  loss: 0.001880435273051262\n",
            " iteration: 435  loss: 0.0018680840730667114\n",
            " iteration: 436  loss: 0.0018613279098644853\n",
            " iteration: 437  loss: 0.0018544754711911082\n",
            " iteration: 438  loss: 0.0018492450471967459\n",
            " iteration: 439  loss: 0.0018441425636410713\n",
            " iteration: 440  loss: 0.0018401297274976969\n",
            " iteration: 441  loss: 0.0018352637998759747\n",
            " iteration: 442  loss: 0.0018289897125214338\n",
            " iteration: 443  loss: 0.0018212390132248402\n",
            " iteration: 444  loss: 0.0018119469750672579\n",
            " iteration: 445  loss: 0.0018019431736320257\n",
            " iteration: 446  loss: 0.0018074896652251482\n",
            " iteration: 447  loss: 0.0017959331162273884\n",
            " iteration: 448  loss: 0.001779664307832718\n",
            " iteration: 449  loss: 0.0017677654977887869\n",
            " iteration: 450  loss: 0.0017565228044986725\n",
            " iteration: 451  loss: 0.0017516512889415026\n",
            " iteration: 452  loss: 0.001747125294059515\n",
            " iteration: 453  loss: 0.0017364663071930408\n",
            " iteration: 454  loss: 0.001731762895360589\n",
            " iteration: 455  loss: 0.0017238145228475332\n",
            " iteration: 456  loss: 0.0017190007492899895\n",
            " iteration: 457  loss: 0.0017140854615718126\n",
            " iteration: 458  loss: 0.0017097966047003865\n",
            " iteration: 459  loss: 0.0017047220608219504\n",
            " iteration: 460  loss: 0.0016953687882050872\n",
            " iteration: 461  loss: 0.00168557558208704\n",
            " iteration: 462  loss: 0.0016876969020813704\n",
            " iteration: 463  loss: 0.0016804977785795927\n",
            " iteration: 464  loss: 0.0016738544218242168\n",
            " iteration: 465  loss: 0.001660630339756608\n",
            " iteration: 466  loss: 0.0016505438834428787\n",
            " iteration: 467  loss: 0.0016217062948271632\n",
            " iteration: 468  loss: 0.0016029960243031383\n",
            " iteration: 469  loss: 0.001590720727108419\n",
            " iteration: 470  loss: 0.0015841794665902853\n",
            " iteration: 471  loss: 0.0015783600974828005\n",
            " iteration: 472  loss: 0.0015717329224571586\n",
            " iteration: 473  loss: 0.001563181634992361\n",
            " iteration: 474  loss: 0.0015744228148832917\n",
            " iteration: 475  loss: 0.0015581767074763775\n",
            " iteration: 476  loss: 0.0015516760759055614\n",
            " iteration: 477  loss: 0.0015436087269335985\n",
            " iteration: 478  loss: 0.0015346137806773186\n",
            " iteration: 479  loss: 0.0015240029897540808\n",
            " iteration: 480  loss: 0.0015163872158154845\n",
            " iteration: 481  loss: 0.0015234111342579126\n",
            " iteration: 482  loss: 0.001508763525635004\n",
            " iteration: 483  loss: 0.001502722967416048\n",
            " iteration: 484  loss: 0.0014981550630182028\n",
            " iteration: 485  loss: 0.0014922416303306818\n",
            " iteration: 486  loss: 0.0014865281991660595\n",
            " iteration: 487  loss: 0.0014853188768029213\n",
            " iteration: 488  loss: 0.0014792820438742638\n",
            " iteration: 489  loss: 0.001475663622841239\n",
            " iteration: 490  loss: 0.0014720673207193613\n",
            " iteration: 491  loss: 0.0014685927890241146\n",
            " iteration: 492  loss: 0.0014629082288593054\n",
            " iteration: 493  loss: 0.0014551060739904642\n",
            " iteration: 494  loss: 0.0016239555552601814\n",
            " iteration: 495  loss: 0.0014511672779917717\n",
            " iteration: 496  loss: 0.0014439771184697747\n",
            " iteration: 497  loss: 0.0014370570424944162\n",
            " iteration: 498  loss: 0.0014317948371171951\n",
            " iteration: 499  loss: 0.0014248217921704054\n",
            " iteration: 500  loss: 0.001417491934262216\n",
            " iteration: 501  loss: 0.0014081487897783518\n",
            " iteration: 502  loss: 0.0014011985622346401\n",
            " iteration: 503  loss: 0.0013923845253884792\n",
            " iteration: 504  loss: 0.0013768987264484167\n",
            " iteration: 505  loss: 0.0013696728274226189\n",
            " iteration: 506  loss: 0.0013616967480629683\n",
            " iteration: 507  loss: 0.0013514251913875341\n",
            " iteration: 508  loss: 0.0013272759970277548\n",
            " iteration: 509  loss: 0.0013111595762893558\n",
            " iteration: 510  loss: 0.001335185021162033\n",
            " iteration: 511  loss: 0.0012990153627470136\n",
            " iteration: 512  loss: 0.0012840303825214505\n",
            " iteration: 513  loss: 0.001301696291193366\n",
            " iteration: 514  loss: 0.0012781682889908552\n",
            " iteration: 515  loss: 0.0012654111487790942\n",
            " iteration: 516  loss: 0.0012525728670880198\n",
            " iteration: 517  loss: 0.00124289037194103\n",
            " iteration: 518  loss: 0.00124254054389894\n",
            " iteration: 519  loss: 0.0012358063831925392\n",
            " iteration: 520  loss: 0.0012253794120624661\n",
            " iteration: 521  loss: 0.0012174296425655484\n",
            " iteration: 522  loss: 0.0012119498569518328\n",
            " iteration: 523  loss: 0.0012077402789145708\n",
            " iteration: 524  loss: 0.0011950326152145863\n",
            " iteration: 525  loss: 0.0011900067329406738\n",
            " iteration: 526  loss: 0.0011853405740112066\n",
            " iteration: 527  loss: 0.0011817291378974915\n",
            " iteration: 528  loss: 0.001178787206299603\n",
            " iteration: 529  loss: 0.0011764147784560919\n",
            " iteration: 530  loss: 0.0011705340584740043\n",
            " iteration: 531  loss: 0.00116367032751441\n",
            " iteration: 532  loss: 0.0011857162462547421\n",
            " iteration: 533  loss: 0.0011568829650059342\n",
            " iteration: 534  loss: 0.0011508518364280462\n",
            " iteration: 535  loss: 0.001136972103267908\n",
            " iteration: 536  loss: 0.0011239333543926477\n",
            " iteration: 537  loss: 0.0011141811264678836\n",
            " iteration: 538  loss: 0.00110725499689579\n",
            " iteration: 539  loss: 0.0011006058193743229\n",
            " iteration: 540  loss: 0.0010909026023000479\n",
            " iteration: 541  loss: 0.0010838407324627042\n",
            " iteration: 542  loss: 0.0010744220344349742\n",
            " iteration: 543  loss: 0.001061836606822908\n",
            " iteration: 544  loss: 0.001050538499839604\n",
            " iteration: 545  loss: 0.0010387887014076114\n",
            " iteration: 546  loss: 0.0010331952944397926\n",
            " iteration: 547  loss: 0.0010287456680089235\n",
            " iteration: 548  loss: 0.0010252580977976322\n",
            " iteration: 549  loss: 0.0010218988172709942\n",
            " iteration: 550  loss: 0.0010174103081226349\n",
            " iteration: 551  loss: 0.001011178595945239\n",
            " iteration: 552  loss: 0.0010036092717200518\n",
            " iteration: 553  loss: 0.0009928334038704634\n",
            " iteration: 554  loss: 0.0009973817504942417\n",
            " iteration: 555  loss: 0.0009874210227280855\n",
            " iteration: 556  loss: 0.0009792454075068235\n",
            " iteration: 557  loss: 0.0009742906549945474\n",
            " iteration: 558  loss: 0.0009683242533355951\n",
            " iteration: 559  loss: 0.0009590390254743397\n",
            " iteration: 560  loss: 0.0009470712393522263\n",
            " iteration: 561  loss: 0.0009402839932590723\n",
            " iteration: 562  loss: 0.000923408311791718\n",
            " iteration: 563  loss: 0.0009149594698101282\n",
            " iteration: 564  loss: 0.0009028789936564863\n",
            " iteration: 565  loss: 0.0008973955409601331\n",
            " iteration: 566  loss: 0.000891917385160923\n",
            " iteration: 567  loss: 0.0008898619562387466\n",
            " iteration: 568  loss: 0.0008865985437296331\n",
            " iteration: 569  loss: 0.0008861812530085444\n",
            " iteration: 570  loss: 0.0011867140419781208\n",
            " iteration: 571  loss: 0.0008852266473695636\n",
            " iteration: 572  loss: 0.0008811397128738463\n",
            " iteration: 573  loss: 0.000876742706168443\n",
            " iteration: 574  loss: 0.0008687283261679113\n",
            " iteration: 575  loss: 0.0008549878257326782\n",
            " iteration: 576  loss: 0.0008404958061873913\n",
            " iteration: 577  loss: 0.0008193569956347346\n",
            " iteration: 578  loss: 0.0008076421217992902\n",
            " iteration: 579  loss: 0.0007928790291771293\n",
            " iteration: 580  loss: 0.0007789981900714338\n",
            " iteration: 581  loss: 0.0008873803308233619\n",
            " iteration: 582  loss: 0.000776731176301837\n",
            " iteration: 583  loss: 0.0007699253037571907\n",
            " iteration: 584  loss: 0.0007644320139661431\n",
            " iteration: 585  loss: 0.0007560771191492677\n",
            " iteration: 586  loss: 0.0007506892434321344\n",
            " iteration: 587  loss: 0.0007469939300790429\n",
            " iteration: 588  loss: 0.0007442663772962987\n",
            " iteration: 589  loss: 0.0007418290479108691\n",
            " iteration: 590  loss: 0.0007407450466416776\n",
            " iteration: 591  loss: 0.0007400347967632115\n",
            " iteration: 592  loss: 0.0007390883984044194\n",
            " iteration: 593  loss: 0.0007371822139248252\n",
            " iteration: 594  loss: 0.000733623921405524\n",
            " iteration: 595  loss: 0.0007290368084795773\n",
            " iteration: 596  loss: 0.0007309530628845096\n",
            " iteration: 597  loss: 0.0007261697319336236\n",
            " iteration: 598  loss: 0.0007223838474601507\n",
            " iteration: 599  loss: 0.0007187454029917717\n",
            " iteration: 600  loss: 0.000717057497240603\n",
            " iteration: 601  loss: 0.0007258804398588836\n",
            " iteration: 602  loss: 0.0007155430503189564\n",
            " iteration: 603  loss: 0.0007129815057851374\n",
            " iteration: 604  loss: 0.0007092895684763789\n",
            " iteration: 605  loss: 0.0007054431480355561\n",
            " iteration: 606  loss: 0.0006974827847443521\n",
            " iteration: 607  loss: 0.0006960661266930401\n",
            " iteration: 608  loss: 0.000691048102453351\n",
            " iteration: 609  loss: 0.0006886528572067618\n",
            " iteration: 610  loss: 0.0006871990044601262\n",
            " iteration: 611  loss: 0.0006858131964690983\n",
            " iteration: 612  loss: 0.0006830834900029004\n",
            " iteration: 613  loss: 0.0006791598862037063\n",
            " iteration: 614  loss: 0.0006741442484781146\n",
            " iteration: 615  loss: 0.0006696929922327399\n",
            " iteration: 616  loss: 0.0006660039653070271\n",
            " iteration: 617  loss: 0.0006627776310779154\n",
            " iteration: 618  loss: 0.0006634733872488141\n",
            " iteration: 619  loss: 0.0006613010191358626\n",
            " iteration: 620  loss: 0.000659696408547461\n",
            " iteration: 621  loss: 0.0006562746712006629\n",
            " iteration: 622  loss: 0.0006538107991218567\n",
            " iteration: 623  loss: 0.0006545441574417055\n",
            " iteration: 624  loss: 0.0006522891344502568\n",
            " iteration: 625  loss: 0.0006493915570899844\n",
            " iteration: 626  loss: 0.0006476741982623935\n",
            " iteration: 627  loss: 0.0006446139886975288\n",
            " iteration: 628  loss: 0.0006409408524632454\n",
            " iteration: 629  loss: 0.0006361798150464892\n",
            " iteration: 630  loss: 0.0006328343879431486\n",
            " iteration: 631  loss: 0.0006295288912951946\n",
            " iteration: 632  loss: 0.0006282007088884711\n",
            " iteration: 633  loss: 0.0006249971338547766\n",
            " iteration: 634  loss: 0.0006238800124265254\n",
            " iteration: 635  loss: 0.000622869876679033\n",
            " iteration: 636  loss: 0.0006212308653630316\n",
            " iteration: 637  loss: 0.0006188447005115449\n",
            " iteration: 638  loss: 0.0006163333309814334\n",
            " iteration: 639  loss: 0.000613901240285486\n",
            " iteration: 640  loss: 0.0006104531930759549\n",
            " iteration: 641  loss: 0.0006082256441004574\n",
            " iteration: 642  loss: 0.000603652442805469\n",
            " iteration: 643  loss: 0.0006017747800797224\n",
            " iteration: 644  loss: 0.0005981014692224562\n",
            " iteration: 645  loss: 0.0006087994552217424\n",
            " iteration: 646  loss: 0.0005969851044937968\n",
            " iteration: 647  loss: 0.0005939043476246297\n",
            " iteration: 648  loss: 0.0005945289740338922\n",
            " iteration: 649  loss: 0.00059109041467309\n",
            " iteration: 650  loss: 0.0005869503947906196\n",
            " iteration: 651  loss: 0.0005834128241986036\n",
            " iteration: 652  loss: 0.0005821490776725113\n",
            " iteration: 653  loss: 0.0005803024396300316\n",
            " iteration: 654  loss: 0.000577795784920454\n",
            " iteration: 655  loss: 0.0005707162199541926\n",
            " iteration: 656  loss: 0.0005622276803478599\n",
            " iteration: 657  loss: 0.0008533857762813568\n",
            " iteration: 658  loss: 0.000557806808501482\n",
            " iteration: 659  loss: 0.0005531641654670238\n",
            " iteration: 660  loss: 0.0006233290187083185\n",
            " iteration: 661  loss: 0.0005481158732436597\n",
            " iteration: 662  loss: 0.0005365941324271262\n",
            " iteration: 663  loss: 0.0005253711133264005\n",
            " iteration: 664  loss: 0.0005177006823942065\n",
            " iteration: 665  loss: 0.0005120395217090845\n",
            " iteration: 666  loss: 0.0005065450677648187\n",
            " iteration: 667  loss: 0.0005022903787903488\n",
            " iteration: 668  loss: 0.0005001391982659698\n",
            " iteration: 669  loss: 0.0004995553754270077\n",
            " iteration: 670  loss: 0.0004967320710420609\n",
            " iteration: 671  loss: 0.0004944056272506714\n",
            " iteration: 672  loss: 0.000492965686134994\n",
            " iteration: 673  loss: 0.0004916833713650703\n",
            " iteration: 674  loss: 0.0004917559563182294\n",
            " iteration: 675  loss: 0.0004891513963229954\n",
            " iteration: 676  loss: 0.0004858920001424849\n",
            " iteration: 677  loss: 0.0004833211423829198\n",
            " iteration: 678  loss: 0.00048149190843105316\n",
            " iteration: 679  loss: 0.00047949678264558315\n",
            " iteration: 680  loss: 0.0004781112656928599\n",
            " iteration: 681  loss: 0.00047595694195479155\n",
            " iteration: 682  loss: 0.0004747917118947953\n",
            " iteration: 683  loss: 0.00047150347381830215\n",
            " iteration: 684  loss: 0.0004683224833570421\n",
            " iteration: 685  loss: 0.0004656001983676106\n",
            " iteration: 686  loss: 0.00046363481669686735\n",
            " iteration: 687  loss: 0.00046065705828368664\n",
            " iteration: 688  loss: 0.0004562825197353959\n",
            " iteration: 689  loss: 0.000453426648164168\n",
            " iteration: 690  loss: 0.00045087235048413277\n",
            " iteration: 691  loss: 0.0004475255263969302\n",
            " iteration: 692  loss: 0.000442316202679649\n",
            " iteration: 693  loss: 0.00043696770444512367\n",
            " iteration: 694  loss: 0.00043341837590560317\n",
            " iteration: 695  loss: 0.00043144659139215946\n",
            " iteration: 696  loss: 0.00042931921780109406\n",
            " iteration: 697  loss: 0.00042715977178886533\n",
            " iteration: 698  loss: 0.0004234930966049433\n",
            " iteration: 699  loss: 0.0004197701928205788\n",
            " iteration: 700  loss: 0.00041689773206599057\n",
            " iteration: 701  loss: 0.0004154428606852889\n",
            " iteration: 702  loss: 0.000414010341046378\n",
            " iteration: 703  loss: 0.0004124839906580746\n",
            " iteration: 704  loss: 0.00041082402458414435\n",
            " iteration: 705  loss: 0.0004098069912288338\n",
            " iteration: 706  loss: 0.0004079763893969357\n",
            " iteration: 707  loss: 0.00040631883894093335\n",
            " iteration: 708  loss: 0.00040562846697866917\n",
            " iteration: 709  loss: 0.0004048479604534805\n",
            " iteration: 710  loss: 0.00040398232522420585\n",
            " iteration: 711  loss: 0.0004031702992506325\n",
            " iteration: 712  loss: 0.00040170963620767\n",
            " iteration: 713  loss: 0.00039905362064018846\n",
            " iteration: 714  loss: 0.00039698806358501315\n",
            " iteration: 715  loss: 0.0003971098340116441\n",
            " iteration: 716  loss: 0.0003961806942243129\n",
            " iteration: 717  loss: 0.00039517783443443477\n",
            " iteration: 718  loss: 0.0003939713351428509\n",
            " iteration: 719  loss: 0.00039292697329074144\n",
            " iteration: 720  loss: 0.00039218858000822365\n",
            " iteration: 721  loss: 0.00039100664434954524\n",
            " iteration: 722  loss: 0.00039046889287419617\n",
            " iteration: 723  loss: 0.00038939545629546046\n",
            " iteration: 724  loss: 0.0003881067386828363\n",
            " iteration: 725  loss: 0.00038659217534586787\n",
            " iteration: 726  loss: 0.00038484271499328315\n",
            " iteration: 727  loss: 0.0003900849260389805\n",
            " iteration: 728  loss: 0.00038419588236138225\n",
            " iteration: 729  loss: 0.0003829681081697345\n",
            " iteration: 730  loss: 0.00038267395575530827\n",
            " iteration: 731  loss: 0.00038079649675637484\n",
            " iteration: 732  loss: 0.00038008339470252395\n",
            " iteration: 733  loss: 0.0003784916771110147\n",
            " iteration: 734  loss: 0.00037690799217671156\n",
            " iteration: 735  loss: 0.00037531761336140335\n",
            " iteration: 736  loss: 0.00037351733772084117\n",
            " iteration: 737  loss: 0.00037364120362326503\n",
            " iteration: 738  loss: 0.0003722963738255203\n",
            " iteration: 739  loss: 0.0003710629534907639\n",
            " iteration: 740  loss: 0.00036987621570006013\n",
            " iteration: 741  loss: 0.0003693594771903008\n",
            " iteration: 742  loss: 0.00036856005317531526\n",
            " iteration: 743  loss: 0.00036728783743456006\n",
            " iteration: 744  loss: 0.0003652492305263877\n",
            " iteration: 745  loss: 0.00036694330628961325\n",
            " iteration: 746  loss: 0.00036462268326431513\n",
            " iteration: 747  loss: 0.0003636210458353162\n",
            " iteration: 748  loss: 0.0003620005154516548\n",
            " iteration: 749  loss: 0.0003601237840484828\n",
            " iteration: 750  loss: 0.00035850692074745893\n",
            " iteration: 751  loss: 0.00035742868203669786\n",
            " iteration: 752  loss: 0.00035674020182341337\n",
            " iteration: 753  loss: 0.0003555922594387084\n",
            " iteration: 754  loss: 0.000354546558810398\n",
            " iteration: 755  loss: 0.0003533724811859429\n",
            " iteration: 756  loss: 0.0003523158375173807\n",
            " iteration: 757  loss: 0.0003503267653286457\n",
            " iteration: 758  loss: 0.0003480806481093168\n",
            " iteration: 759  loss: 0.00034626369597390294\n",
            " iteration: 760  loss: 0.00034489863901399076\n",
            " iteration: 761  loss: 0.0003442949091549963\n",
            " iteration: 762  loss: 0.0003446577466093004\n",
            " iteration: 763  loss: 0.00034399746800772846\n",
            " iteration: 764  loss: 0.00034320345730520785\n",
            " iteration: 765  loss: 0.0003413326048757881\n",
            " iteration: 766  loss: 0.00033798819640651345\n",
            " iteration: 767  loss: 0.00033823587000370026\n",
            " iteration: 768  loss: 0.00033683597575873137\n",
            " iteration: 769  loss: 0.00033258862094953656\n",
            " iteration: 770  loss: 0.0003294067573733628\n",
            " iteration: 771  loss: 0.0003260844387114048\n",
            " iteration: 772  loss: 0.0003232104063499719\n",
            " iteration: 773  loss: 0.00032081661629490554\n",
            " iteration: 774  loss: 0.0003201263607479632\n",
            " iteration: 775  loss: 0.000317867670673877\n",
            " iteration: 776  loss: 0.0003171626594848931\n",
            " iteration: 777  loss: 0.00031616337946616113\n",
            " iteration: 778  loss: 0.0003353951033204794\n",
            " iteration: 779  loss: 0.00031578511698171496\n",
            " iteration: 780  loss: 0.000315329700242728\n",
            " iteration: 781  loss: 0.0003148566756863147\n",
            " iteration: 782  loss: 0.0003145522787235677\n",
            " iteration: 783  loss: 0.00031406377092935145\n",
            " iteration: 784  loss: 0.0003136807936243713\n",
            " iteration: 785  loss: 0.0003132015117444098\n",
            " iteration: 786  loss: 0.00031256931833922863\n",
            " iteration: 787  loss: 0.00031244001002050936\n",
            " iteration: 788  loss: 0.00031136878533288836\n",
            " iteration: 789  loss: 0.00031111371936276555\n",
            " iteration: 790  loss: 0.0003105720388703048\n",
            " iteration: 791  loss: 0.00030991988023743033\n",
            " iteration: 792  loss: 0.0003089877136517316\n",
            " iteration: 793  loss: 0.0003083098563365638\n",
            " iteration: 794  loss: 0.00030966149643063545\n",
            " iteration: 795  loss: 0.00030808881274424493\n",
            " iteration: 796  loss: 0.000307572161545977\n",
            " iteration: 797  loss: 0.00030720222275704145\n",
            " iteration: 798  loss: 0.0003066564677283168\n",
            " iteration: 799  loss: 0.0003060459566768259\n",
            " iteration: 800  loss: 0.0003045139601454139\n",
            " iteration: 801  loss: 0.0003034252149518579\n",
            " iteration: 802  loss: 0.0003023853641934693\n",
            " iteration: 803  loss: 0.0003015771508216858\n",
            " iteration: 804  loss: 0.00030055129900574684\n",
            " iteration: 805  loss: 0.00030016701202839613\n",
            " iteration: 806  loss: 0.00029804304358549416\n",
            " iteration: 807  loss: 0.0002969079650938511\n",
            " iteration: 808  loss: 0.0002958316181320697\n",
            " iteration: 809  loss: 0.00029403052758425474\n",
            " iteration: 810  loss: 0.0002931504277512431\n",
            " iteration: 811  loss: 0.0002923168067354709\n",
            " iteration: 812  loss: 0.0002916489902418107\n",
            " iteration: 813  loss: 0.00029118009842932224\n",
            " iteration: 814  loss: 0.00029078146326355636\n",
            " iteration: 815  loss: 0.00029042799724265933\n",
            " iteration: 816  loss: 0.00029000832000747323\n",
            " iteration: 817  loss: 0.0002896485384553671\n",
            " iteration: 818  loss: 0.0002915723016485572\n",
            " iteration: 819  loss: 0.0002892322954721749\n",
            " iteration: 820  loss: 0.00028876514988951385\n",
            " iteration: 821  loss: 0.0002872738696169108\n",
            " iteration: 822  loss: 0.0002866008726414293\n",
            " iteration: 823  loss: 0.000285703397821635\n",
            " iteration: 824  loss: 0.00029279221780598164\n",
            " iteration: 825  loss: 0.00028540444327518344\n",
            " iteration: 826  loss: 0.000284840352833271\n",
            " iteration: 827  loss: 0.0002829905424732715\n",
            " iteration: 828  loss: 0.0002814119216054678\n",
            " iteration: 829  loss: 0.00028028388624079525\n",
            " iteration: 830  loss: 0.00027955096447840333\n",
            " iteration: 831  loss: 0.0002790114958770573\n",
            " iteration: 832  loss: 0.0002784879761748016\n",
            " iteration: 833  loss: 0.0002781319781206548\n",
            " iteration: 834  loss: 0.0002776695473585278\n",
            " iteration: 835  loss: 0.0002779240603558719\n",
            " iteration: 836  loss: 0.0002772102307062596\n",
            " iteration: 837  loss: 0.00027642512577585876\n",
            " iteration: 838  loss: 0.0002754541055765003\n",
            " iteration: 839  loss: 0.0002748380065895617\n",
            " iteration: 840  loss: 0.0003015557595063001\n",
            " iteration: 841  loss: 0.00027431012131273746\n",
            " iteration: 842  loss: 0.00027369812596589327\n",
            " iteration: 843  loss: 0.0002730866544879973\n",
            " iteration: 844  loss: 0.0002726682578213513\n",
            " iteration: 845  loss: 0.00027178943855687976\n",
            " iteration: 846  loss: 0.0002697749005164951\n",
            " iteration: 847  loss: 0.00026838554185815156\n",
            " iteration: 848  loss: 0.00026716446154750884\n",
            " iteration: 849  loss: 0.0002660912286955863\n",
            " iteration: 850  loss: 0.0002652898838277906\n",
            " iteration: 851  loss: 0.0002646772190928459\n",
            " iteration: 852  loss: 0.0002641335013322532\n",
            " iteration: 853  loss: 0.0002702728961594403\n",
            " iteration: 854  loss: 0.00026356789749115705\n",
            " iteration: 855  loss: 0.000263210735283792\n",
            " iteration: 856  loss: 0.0002629357622936368\n",
            " iteration: 857  loss: 0.0002627386129461229\n",
            " iteration: 858  loss: 0.0002619496954139322\n",
            " iteration: 859  loss: 0.0002646655193530023\n",
            " iteration: 860  loss: 0.0002615088305901736\n",
            " iteration: 861  loss: 0.00026042881654575467\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "import torch\n",
        "from main import BurgersNN\n",
        "\n",
        "# load model\n",
        "model = BurgersNN()\n",
        "model.load_state_dict(torch.load('Burgers_Equation/models/model_LBFGS_shuffle_normal.pt'))\n",
        "x = torch.linspace(-1, 1, 200)\n",
        "t = torch.linspace( 0, 1, 100)\n",
        "\n",
        "# x & t grids:\n",
        "X, T = torch.meshgrid(x, t)\n",
        "\n",
        "# x & t columns:\n",
        "xcol = X.reshape(-1, 1)\n",
        "tcol = T.reshape(-1, 1)\n",
        "input = torch.cat((xcol, tcol), 1)\n",
        "# one large column:\n",
        "usol = model(input)\n",
        "\n",
        "# reshape solution:\n",
        "U = usol.reshape(x.numel(), t.numel())\n",
        "\n",
        "# transform to numpy:\n",
        "xnp = x.numpy()\n",
        "tnp = t.numpy()\n",
        "Unp = U.detach().numpy()\n",
        "\n",
        "# plot:\n",
        "fig = plt.figure(figsize=(9, 4.5))\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "h = ax.imshow(Unp,\n",
        "                interpolation='nearest',\n",
        "                cmap='rainbow',\n",
        "                extent=[tnp.min(), tnp.max(), xnp.min(), xnp.max()],\n",
        "                origin='lower', aspect='auto')\n",
        "divider = make_axes_locatable(ax)\n",
        "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.10)\n",
        "cbar = fig.colorbar(h, cax=cax)\n",
        "cbar.ax.tick_params(labelsize=10)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WhuVbh5HhMFn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}